import json
from pathlib import Path
from collections import defaultdict, Counter
import pandas as pd  # type:ignore
import seaborn as sns  # type:ignore
import matplotlib.pyplot as plt  # type:ignore
import numpy as np


def read_similarity_scores(data_version, return_all=False):
    run_ids = [
        "final_zero_vocab",
        "final_50_vocab",
        "final_100_vocab",
        "final_200_vocab",
        "final_300_vocab",
        "final_500_vocab",
    ]
    rewrite_scores = defaultdict(list)

    for run in run_ids:
        if data_version == "final":
            path = Path("..") / "results" / run / f"descriptors_{run}_final.jsonl"
            with path.open("r") as f:
                for line in f:
                    line = json.loads(line, strict=False)
                    rewrite_scores[run].append((line["similarity"][0]))
        elif data_version == "og":
            path = Path("..") / "results" / run / f"descriptors_{run}.jsonl"
            with path.open("r") as f:
                for line in f:
                    line = json.loads(line, strict=False)
                    if return_all:
                        rewrite_scores[run].append(line["similarity"])
                    else:
                        rewrite_scores[run].append(max(line["similarity"]))
    return rewrite_scores


def calculate_avg_rewrite_scores():
    """Calculate the average rewrite quality in the 'final' data versions."""

    rewrite_scores = read_similarity_scores("final")

    save_path = Path("..") / "results" / "evaluations" / "rewrite_score_averages.txt"
    with save_path.open("w") as f:
        for k, v in rewrite_scores.items():
            avg = sum(v) / len(v)
            f.write(f"{k}: {avg}\n")


def basic_stats(og_scores, final_scores):
    # Calculate average scores
    og_avg = round(np.mean(og_scores), 3)
    final_avg = round(np.mean(final_scores), 3)

    # Calculate standard deviation
    og_std = round(np.std(og_scores), 3)
    final_std = round(np.std(final_scores), 3)

    # Calculate max and min
    og_min = round(min(og_scores), 3)
    og_max = round(max(og_scores), 3)
    final_min = round(min(final_scores), 3)
    final_max = round(max(final_scores), 3)

    # Save basic stats
    save_path = (
        Path("..") / "results" / "evaluations" / "og_vs_synonym_rewrite_diffs.txt"
    )
    save_path.parent.mkdir(parents=True, exist_ok=True)
    with save_path.open("w") as f:
        f.write("similarity score stats between original descriptors and synonyms.\n")
        f.write("Calculated using the zero vocab setting.\n\n")
        f.write(
            f"Original descriptors:\nMean: {og_avg}\n"
            f"Standard deviation: {og_std}\n"
            f"Minimum value: {og_min}\n"
            f"Maximum vaue: {og_max}\n\n"
        )
        f.write(
            f"Synonym descriptors:\nMean: {final_avg}\n"
            f"Standard deviation: {final_std}\n"
            f"Minimum value: {final_min}\n"
            f"Maximum vaue: {final_max}\n"
        )


def plot_score_distribution(og_scores, final_scores):
    bins = np.round(np.arange(start=0, stop=1 + 0.05, step=0.05), 2)
    # Create a DataFrame in long format
    df_og = pd.DataFrame({"value": og_scores, "dataset": "Original descriptors"})
    df_final = pd.DataFrame({"value": final_scores, "dataset": "Synonym descriptors"})
    df = pd.concat([df_og, df_final])

    # Bin the data
    df["bin"] = pd.cut(df["value"], bins=bins, include_lowest=True)

    # Count the number of values per bin per dataset
    bin_counts = df.groupby(["dataset", "bin"]).size().reset_index(name="count")

    # To make bin centers for plotting
    bin_counts["bin_label"] = bin_counts["bin"].apply(
        lambda x: f"{x.left:.1f}â€“{x.right:.1f}"
    )

    # Plot using seaborn
    plt.figure(figsize=(10, 6))
    sns.set_theme(style="whitegrid", context="talk")
    sns.barplot(
        data=bin_counts,
        x="bin_label",
        y="count",
        hue="dataset",
        width=0.8,
        palette="viridis",
    )

    plt.xlabel("Similarity score range")
    plt.ylabel("Count")
    plt.xticks(rotation=45, fontsize=10)

    # Save figure
    fig_path = (
        Path("..") / "figures" / "rewrite_score_distribution_in_zero_vocab_setting.png"
    )
    fig_path.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(fig_path, bbox_inches="tight", dpi=300)


def plot_score_differences(og_scores, final_scores):
    # Calculate differences
    diffs = [final - og for og, final in zip(og_scores, final_scores)]
    diffs_sorted = sorted(diffs)

    # Create bins and histogram
    min_diff, max_diff = min(diffs_sorted), max(diffs_sorted)
    bins = np.round(np.arange(min_diff, max_diff + 0.1, 0.1), 1)  # Create bins of 0.1
    bin_counts, bin_edges = np.histogram(diffs_sorted, bins=bins)
    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])
    bin_centers = np.round(bin_centers, 2)

    # Plot setup
    sns.set_theme(style="whitegrid", context="talk")
    plt.figure(dpi=300, figsize=(10, 6))
    ax = sns.barplot(x=bin_centers, y=bin_counts, palette="viridis")

    # Customize plot
    ax.set_xlabel("Change in rewrite similarity", fontsize=14)
    ax.set_ylabel("Document count", fontsize=14)
    ax.tick_params(axis="both", which="major", labelsize=12)

    for i, count in enumerate(bin_counts):
        ax.text(i, count + 1, str(count), ha="center", va="bottom", fontsize=10)

    sns.despine()

    # Save figure
    fig_path = Path("..") / "figures" / "rewrite_diffs_in_zero_vocab_setting.png"
    fig_path.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(fig_path, bbox_inches="tight", dpi=300)


def rewrite_quality_og_vs_synonyms():
    """Calculate the difference between original descriptors and rewrites in the zero vocab setting."""
    path_to_og = (
        Path("..")
        / "results"
        / "final_zero_vocab"
        / "descriptors_final_zero_vocab.jsonl"
    )
    path_to_final = (
        Path("..")
        / "results"
        / "final_zero_vocab"
        / "descriptors_final_zero_vocab_final.jsonl"
    )

    og_scores = []
    final_scores = []

    with path_to_og.open("r") as f:
        for line in f:
            line = json.loads(line, strict=False)
            scores = line["similarity"]
            og_scores.append(max(scores))

    with path_to_final.open("r") as f:
        for line in f:
            line = json.loads(line, strict=False)
            final_scores.append(line["similarity"][0])

    # Write some basic stats to file
    basic_stats(og_scores, final_scores)

    # Plot score distribution
    plot_score_distribution(og_scores, final_scores)

    # Plot score differences
    plot_score_differences(og_scores, final_scores)


def plot_effect_of_revision_rounds():

    rewrite_scores = read_similarity_scores("og", return_all=True)

    num_plots = len(rewrite_scores)
    cols = 3
    rows = (num_plots + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))
    axes = axes.flatten()

    for idx, (run, scores) in enumerate(rewrite_scores.items()):
        max_indices = [np.argmax(sim) for sim in scores]
        count_dict = Counter(max_indices)
        x = list(count_dict.keys())
        y = list(count_dict.values())

        sns.barplot(x=x, y=y, palette="viridis", edgecolor="black", ax=axes[idx])
        axes[idx].set_title(run)
        axes[idx].set_xlabel("Best rewrite index")

        for i, val in zip(x, y):
            axes[idx].text(i, val, str(val), ha="center", va="bottom")

    # Hide any unused subplots
    for ax in axes[num_plots:]:
        ax.set_visible(False)

    plt.tight_layout()
    plt.savefig("../figures/revision_effects_grid.png")


if __name__ == "__main__":
    calculate_avg_rewrite_scores()
    rewrite_quality_og_vs_synonyms()
    plot_effect_of_revision_rounds()
